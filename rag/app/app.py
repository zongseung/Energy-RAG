import re
import os
import sys
import time
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import streamlit as st
from langchain_core.messages.chat import ChatMessage
from core.graph import build_graph

st.set_page_config(page_title="Energy-gpt", layout="wide")
st.title("Energy-gptì— ì˜¤ì‹ ê±¸ í™˜ì˜í•©ë‹ˆë‹¤.")

# ëŒ€í™” ì €ì¥ì†Œ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì‚¬ì´ë“œë°”
with st.sidebar:
    if st.button("ëŒ€í™”ì´ˆê¸°í™”"):
        st.session_state["messages"] = []

# ë©”ì‹œì§€ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

print_messages()

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.chat_message("user").write(user_input)

    # LangGraph ì‹¤í–‰
    graph = build_graph()
    ai_answer = ""

    with st.chat_message("assistant"):
        container = st.empty()
        thinking_container = st.empty()
        
        # ìµœê·¼ 3í„´(history) êµ¬ì„±: user-assistant í˜ì–´ ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ 3ê°œ ë¬¶ìŒ
        msgs = st.session_state["messages"]
        pairs = []
        i = len(msgs) - 1
        # messagesëŠ” [user, assistant, user, assistant, ...] í˜•íƒœë¡œ ì €ì¥ë¨
        while i >= 1 and len(pairs) < 3:
            if msgs[i-1].role == "user" and msgs[i].role == "assistant":
                pairs.append({
                    "user": msgs[i-1].content,
                    "assistant": msgs[i].content,
                })
                i -= 2
            else:
                i -= 1

        history = []
        # ìµœê·¼ ê²ƒì´ ë¨¼ì €ì˜€ìœ¼ë¯€ë¡œ ì—­ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì „ë‹¬
        for p in reversed(pairs):
            history.append({"role": "user", "content": p["user"]})
            history.append({"role": "assistant", "content": p["assistant"]})

        # LangGraph ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰
        final_output = {}
        node_messages = {
            "router": "ğŸ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "retriever": "ğŸ“š ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "supervisor": "ğŸ¤” ì „ë¬¸ê°€ë¥¼ ì„ íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "energy_industry_agent": "âš¡ ì—ë„ˆì§€ì‚°ì—…ë¶„ì„ì „ë¬¸ê°€ê°€ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "renewable_energy_agent": "ğŸŒ± ì¬ìƒì—ë„ˆì§€ë¶„ì„ì „ë¬¸ê°€ê°€ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "reflection_agent": "ğŸ§  ë‹µë³€ì„ ì„±ì°°í•˜ê³  ê°œì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "explainer": "ğŸ¯ ìµœì¢… ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
        }
        
        try:
            for chunk in graph.stream({"query": user_input, "history": history}):
                for node_name, node_output in chunk.items():
                    if node_name in node_messages:
                        thinking_container.markdown(
                            f"<small style='color: gray;'>{node_messages[node_name]}</small>", 
                            unsafe_allow_html=True
                        )
                    final_output.update(node_output)
        except Exception as e:
            thinking_container.markdown(
                f"<small style='color: red;'>âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}</small>", 
                unsafe_allow_html=True
            )
            final_output = {"final": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
        
        # íƒ€ìê¸° íš¨ê³¼ë¡œ ìµœì¢… ë‹µë³€ í‘œì‹œ
        ai_answer = final_output.get("final", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        thinking_container.markdown(
            f"<small style='color: gray;'>ğŸ’¬ ë‹µë³€ì„ ì¶œë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...</small>", 
            unsafe_allow_html=True
        )
        
        # íƒ€ìê¸° íš¨ê³¼
        displayed_text = ""
        for i, char in enumerate(ai_answer):
            displayed_text += char
            container.markdown(displayed_text + "â–Š", unsafe_allow_html=True)
            if i % 3 == 0:  # 3ê¸€ìë§ˆë‹¤ ì—…ë°ì´íŠ¸
                time.sleep(0.01)
        
        thinking_container.empty()  # ìƒê° ì¤‘ ë©”ì‹œì§€ ì œê±°
        container.markdown(ai_answer, unsafe_allow_html=True)  # ìµœì¢… ì»¤ì„œ ì œê±°
        
        # ìµœì¢… outputì„ out ë³€ìˆ˜ì— í• ë‹¹ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€)
        out = final_output

        # ë¬¸ì„œ ì¶œì²˜ í‘œì‹œ (ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ)
        candidates = out.get("candidates", [])
        if candidates:
            st.write("---")
            st.subheader("ì°¸ê³  ë¬¸ì„œ")
            for i, candidate in enumerate(candidates[:8], 1):
                filename = candidate.get('filename', 'N/A')
                page = candidate.get('page', 'N/A')
                chunk_type = candidate.get('chunk_type', 'N/A')
                content = candidate.get('content', '')
                
                st.write(f"**{i}.** {filename} (í˜ì´ì§€ {page}, {chunk_type})")
                
                # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ ë° í‘œì‹œ
                image_paths = re.findall(r'!\[.*?\]\((.*?)\)', content)
                for img_path in image_paths:
                    if img_path.startswith("/static/"):
                        st.markdown(f"![ê·¸ë¦¼]({img_path})", unsafe_allow_html=True)
                    elif os.path.exists(img_path):
                        st.image(img_path, caption=os.path.basename(img_path))

        # í…Œì´ë¸” ê²°ê³¼ í‘œì‹œ
        if out.get("result", {}).get("type") == "table":
            import pandas as pd
            df = pd.DataFrame(out["result"].get("preview", []))
            st.dataframe(df, use_container_width=True)

    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
    st.session_state["messages"].append(ChatMessage(role="assistant", content=ai_answer))